{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1984ecba",
   "metadata": {},
   "source": [
    "1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a625345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import hashlib\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbe0d4e",
   "metadata": {},
   "source": [
    "2. Dataset Freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6cb0d9",
   "metadata": {},
   "source": [
    "All preprocessing will be performed on dataset_working, while dataset_raw remained unchanged as a frozen reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d847145a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Creating dataset_raw (frozen copy)...\n",
      "üì¶ Creating dataset_working (preprocessing copy)...\n",
      "‚Ä¢ dataset_raw      ‚Üí DO NOT TOUCH\n",
      "‚Ä¢ dataset_working  ‚Üí use for preprocessing\n"
     ]
    }
   ],
   "source": [
    "SRC_DATASET = Path(\"dataset\")\n",
    "RAW_DATASET = Path(\"dataset_raw\")\n",
    "WORK_DATASET = Path(\"dataset_working\")\n",
    "\n",
    "\n",
    "if RAW_DATASET.exists() or WORK_DATASET.exists():\n",
    "    raise RuntimeError(\n",
    "        \"dataset_raw or dataset_working already exists.\\n\"\n",
    "        \"Delete them manually if you want to re-freeze.\"\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"üì¶ Creating dataset_raw (frozen copy)...\")\n",
    "shutil.copytree(SRC_DATASET, RAW_DATASET)\n",
    "\n",
    "print(\"üì¶ Creating dataset_working (preprocessing copy)...\")\n",
    "shutil.copytree(SRC_DATASET, WORK_DATASET)\n",
    "\n",
    "print(\"‚Ä¢ dataset_raw      ‚Üí DO NOT TOUCH\")\n",
    "print(\"‚Ä¢ dataset_working  ‚Üí use for preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7473928",
   "metadata": {},
   "source": [
    "3. Structural Consistency Fix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d63f3",
   "metadata": {},
   "source": [
    "A directory normalization step was applied to enforce a consistent YOLO-compatible structure, ensuring all images and annotation files were organized into standardized images/ and labels/ subdirectories for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "407bddd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Checking: 0 (tempe goreng)\n",
      "‚ñ∂ Checking: 1 (tahu goreng)\n",
      "‚ñ∂ Checking: 10 (nasi goreng)\n",
      "‚ñ∂ Checking: 11 (bubur ayam)\n",
      "‚ñ∂ Checking: 12 (cakwe)\n",
      "‚ñ∂ Checking: 13 (mie ayam)\n",
      "‚ñ∂ Checking: 14 (nasi padang)\n",
      "‚ñ∂ Checking: 15 (babi guling)\n",
      "‚ñ∂ Checking: 16 (nasi uduk)\n",
      "‚ñ∂ Checking: 17 (nasi babi campur)\n",
      "‚ñ∂ Checking: 18 (ayam pop)\n",
      "‚ñ∂ Checking: 19 (telur balado)\n",
      "‚ñ∂ Checking: 2 (rendang)\n",
      "‚ñ∂ Checking: 20 (telur dadar)\n",
      "‚ñ∂ Checking: 21 (telur ceplok)\n",
      "‚ñ∂ Checking: 22 (nasi putih)\n",
      "‚ñ∂ Checking: 23 (dadar gulung)\n",
      "‚ñ∂ Checking: 24 (putu ayu)\n",
      "‚ñ∂ Checking: 25 (kue cubit)\n",
      "‚ñ∂ Checking: 26 (pepes ikan)\n",
      "‚ñ∂ Checking: 27 (bika ambon)\n",
      "‚ñ∂ Checking: 28 (soto)\n",
      "‚ñ∂ Checking: 29 (lumpia)\n",
      "‚ñ∂ Checking: 3 (kangkung)\n",
      "‚ñ∂ Checking: 30 (bihun goreng)\n",
      "‚ñ∂ Checking: 31 (pempek)\n",
      "‚ñ∂ Checking: 32 (batagor)\n",
      "‚ñ∂ Checking: 33 (ikan goreng)\n",
      "‚ñ∂ Checking: 34 (telur rebus)\n",
      "‚ñ∂ Checking: 35 (martabak manis)\n",
      "‚ñ∂ Checking: 36 (gulai ikan)\n",
      "‚ñ∂ Checking: 37 (tempe bacem)\n",
      "‚ñ∂ Checking: 38 (terong balado)\n",
      "‚ñ∂ Checking: 39 (bakwan)\n",
      "‚ñ∂ Checking: 4 (sate)\n",
      "‚ñ∂ Checking: 5 (bakso)\n",
      "‚ñ∂ Checking: 6 (ayam bakar)\n",
      "‚ñ∂ Checking: 7 (ayam goreng)\n",
      "‚ñ∂ Checking: 8 (gado gado)\n",
      "‚ñ∂ Checking: 9 (mie goreng)\n",
      "\n",
      "‚úÖ Dataset structure normalization completed successfully.\n",
      "All image and label files are now organized into 'images/' and 'labels/' folders.\n"
     ]
    }
   ],
   "source": [
    "DATASET_ROOT = Path(\"dataset_working\")\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "\n",
    "\n",
    "for cls in sorted(DATASET_ROOT.iterdir()):\n",
    "    if not cls.is_dir():\n",
    "        continue\n",
    "\n",
    "    print(f\"‚ñ∂ Checking: {cls.name}\")\n",
    "\n",
    "    img_dir = cls / \"images\"\n",
    "    lbl_dir = cls / \"labels\"\n",
    "\n",
    "    img_dir.mkdir(exist_ok=True)\n",
    "    lbl_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    for p in cls.iterdir():\n",
    "        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
    "            dest = img_dir / p.name\n",
    "            if not dest.exists():\n",
    "                shutil.move(str(p), str(dest))\n",
    "                print(f\"  ‚úî Moved image ‚Üí images/{p.name}\")\n",
    "\n",
    "    for p in cls.iterdir():\n",
    "        if p.is_file() and p.suffix.lower() == \".txt\":\n",
    "            dest = lbl_dir / p.name\n",
    "            if not dest.exists():\n",
    "                shutil.move(str(p), str(dest))\n",
    "                print(f\"  ‚úî Moved label ‚Üí labels/{p.name}\")\n",
    "\n",
    "print(\"\\n‚úÖ Dataset structure normalization completed successfully.\")\n",
    "print(\"All image and label files are now organized into 'images/' and 'labels/' folders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6ee268",
   "metadata": {},
   "source": [
    "4. Label Cleaning & Validation - Remove Empty Label Files (and Corresponding Images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdfecb2",
   "metadata": {},
   "source": [
    "Label files containing no valid YOLO bounding box annotations were identified and removed. Corresponding images were also deleted to maintain one-to-one consistency between images and labels and to ensure dataset integrity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec6fc34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Image without label: dataset_working\\0 (tempe goreng)\\images\\tempe-goreng-krispy-foto-resep-utama_jpg.rf.ceb0b9fa7925f4c7b2d25f19f0b8a71f.jpg\n",
      "‚ùå Empty label: dataset_working\\35 (martabak manis)\\labels\\apam_balik_173_jpg.rf.6b3e7991e29e6ab05f5e85dcc54f7ace.txt\n",
      "   ‚îî‚îÄ deleting image: dataset_working\\35 (martabak manis)\\images\\apam_balik_173_jpg.rf.6b3e7991e29e6ab05f5e85dcc54f7ace.jpg\n",
      "Images deleted : 2\n",
      "Labels deleted : 1\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATASET_ROOT = Path(\"dataset_working\")\n",
    "LABEL_DIR_NAME = \"labels\"\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "\n",
    "deleted_images = 0\n",
    "deleted_labels = 0\n",
    "\n",
    "\n",
    "for cls in sorted(DATASET_ROOT.iterdir()):\n",
    "    if not cls.is_dir():\n",
    "        continue\n",
    "\n",
    "    img_dir = cls / \"images\"\n",
    "    lbl_dir = cls / LABEL_DIR_NAME\n",
    "    if not img_dir.exists() or not lbl_dir.exists():\n",
    "        continue\n",
    "\n",
    "    images = {}\n",
    "    for p in img_dir.iterdir():\n",
    "        if p.suffix.lower() in IMG_EXTS:\n",
    "            images[p.stem] = p\n",
    "\n",
    "    labels = {p.stem: p for p in lbl_dir.glob(\"*.txt\")}\n",
    "\n",
    "\n",
    "    # Handle empty labels FIRST\n",
    "    # -------------------------\n",
    "    for stem, lbl_path in list(labels.items()):\n",
    "        content = lbl_path.read_text().strip()\n",
    "\n",
    "        if content == \"\":\n",
    "            print(f\"‚ùå Empty label: {lbl_path}\")\n",
    "            lbl_path.unlink()\n",
    "            deleted_labels += 1\n",
    "\n",
    "            img_path = images.get(stem)\n",
    "            if img_path and img_path.exists():\n",
    "                print(f\"   ‚îî‚îÄ deleting image: {img_path}\")\n",
    "                img_path.unlink()\n",
    "                deleted_images += 1\n",
    "\n",
    "            labels.pop(stem, None)\n",
    "            images.pop(stem, None)\n",
    "\n",
    "\n",
    "    # Images without labels\n",
    "    # -------------------------\n",
    "    for stem, img_path in list(images.items()):\n",
    "        if stem not in labels:\n",
    "            print(f\"‚ùå Image without label: {img_path}\")\n",
    "            img_path.unlink()\n",
    "            deleted_images += 1\n",
    "\n",
    "\n",
    "    # Labels without images\n",
    "    # -------------------------\n",
    "    for stem, lbl_path in list(labels.items()):\n",
    "        if stem not in images:\n",
    "            print(f\"‚ùå Label without image: {lbl_path}\")\n",
    "            lbl_path.unlink()\n",
    "            deleted_labels += 1\n",
    "\n",
    "\n",
    "print(f\"Images deleted : {deleted_images}\")\n",
    "print(f\"Labels deleted : {deleted_labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df110e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "\n",
    "for cls in Path(\"dataset_working\").iterdir():\n",
    "    if not cls.is_dir():\n",
    "        continue\n",
    "\n",
    "    img_dir = cls / \"images\"\n",
    "    lbl_dir = cls / \"labels\"\n",
    "    if not img_dir.exists() or not lbl_dir.exists():\n",
    "        continue\n",
    "\n",
    "    img_stems = {\n",
    "        p.stem for p in img_dir.iterdir()\n",
    "        if p.is_file() and p.suffix.lower() in IMG_EXTS\n",
    "    }\n",
    "    lbl_stems = {p.stem for p in lbl_dir.glob(\"*.txt\") if p.is_file()}\n",
    "\n",
    "    missing_lbl = sorted(img_stems - lbl_stems)\n",
    "    missing_img = sorted(lbl_stems - img_stems)\n",
    "\n",
    "    if missing_lbl or missing_img:\n",
    "        print(f\"\\n‚ùå {cls.name}\")\n",
    "        if missing_lbl:\n",
    "            print(f\"  Images without labels: {len(missing_lbl)}\")\n",
    "            print(f\"  e.g. {missing_lbl[:5]}\")\n",
    "        if missing_img:\n",
    "            print(f\"  Labels without images: {len(missing_img)}\")\n",
    "            print(f\"  e.g. {missing_img[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7327235e",
   "metadata": {},
   "source": [
    "5. Remove Duplicate Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a69edb3",
   "metadata": {},
   "source": [
    "Duplicate images were identified using content-based hashing to detect exact binary duplicates within each class. An initial inspection phase was performed to verify duplicate groups without modifying the dataset, followed by a controlled removal step to eliminate redundant images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be50c4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class: 10 (nasi goreng)\n",
      "  Duplicate group:\n",
      "    292731.jpg\n",
      "    292769.jpg\n",
      "\n",
      "Class: 11 (bubur ayam)\n",
      "  Duplicate group:\n",
      "    249097.jpg\n",
      "    249119.jpg\n",
      "\n",
      "Class: 13 (mie ayam)\n",
      "  Duplicate group:\n",
      "    319405.jpg\n",
      "    319409.jpg\n",
      "    319412.jpg\n",
      "    319639.jpg\n",
      "\n",
      "Class: 14 (nasi padang)\n",
      "  Duplicate group:\n",
      "    247871.jpg\n",
      "    269533.jpg\n",
      "\n",
      "Class: 22 (nasi putih)\n",
      "  Duplicate group:\n",
      "    10645.jpg\n",
      "    9117.jpg\n",
      "  Duplicate group:\n",
      "    11156.jpg\n",
      "    11161.jpg\n",
      "\n",
      "Class: 33 (ikan goreng)\n",
      "  Duplicate group:\n",
      "    Ikan-Goreng_419_jpg.rf.322abb1764ba684524eb45af5de518bb.jpg\n",
      "    ikan_train-10-_jpg.rf.8b247a7eae1531418b6d430ca8c72057.jpg\n",
      "\n",
      "Class: 9 (mie goreng)\n",
      "  Duplicate group:\n",
      "    143_233273.jpg\n",
      "    143_271148.jpg\n",
      "\n",
      "üîé CHECK SUMMARY - NO DELETION\n",
      "Duplicate groups found : 8\n",
      "Images to be removed   : 10\n",
      "\n",
      "‚ö†Ô∏è No files were deleted.\n"
     ]
    }
   ],
   "source": [
    "DATASET_ROOT = Path(\"dataset_working\")\n",
    "LABEL_DIR_NAME = \"labels\"\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "\n",
    "\n",
    "def file_hash(path: Path, chunk_size=8192):\n",
    "    h = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while chunk := f.read(chunk_size):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "total_dup_groups = 0\n",
    "total_dup_images = 0\n",
    "\n",
    "for cls in sorted(DATASET_ROOT.iterdir()):\n",
    "    if not cls.is_dir():\n",
    "        continue\n",
    "\n",
    "    img_dir = cls / \"images\"\n",
    "    lbl_dir = cls / LABEL_DIR_NAME\n",
    "    if not img_dir.exists() or not lbl_dir.exists():\n",
    "        continue\n",
    "\n",
    "    hash_map = defaultdict(list)\n",
    "\n",
    "    for img in img_dir.iterdir():\n",
    "        if img.suffix.lower() in IMG_EXTS:\n",
    "            h = file_hash(img)\n",
    "            hash_map[h].append(img)\n",
    "\n",
    "    dup_groups = [imgs for imgs in hash_map.values() if len(imgs) > 1]\n",
    "\n",
    "    if dup_groups:\n",
    "        print(f\"\\nClass: {cls.name}\")\n",
    "        for group in dup_groups:\n",
    "            total_dup_groups += 1\n",
    "            print(\"  Duplicate group:\")\n",
    "            for img in group:\n",
    "                print(\"   \", img.name)\n",
    "            total_dup_images += len(group) - 1\n",
    "\n",
    "print(\"\\nüîé CHECK SUMMARY - NO DELETION\")\n",
    "print(f\"Duplicate groups found : {total_dup_groups}\")\n",
    "print(f\"Images to be removed   : {total_dup_images}\")\n",
    "print(\"\\n‚ö†Ô∏è No files were deleted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c7fc22",
   "metadata": {},
   "source": [
    "After the duplicate inspection step, all exact duplicate image groups were reported without removing any files. This allowed manual verification of detected duplicates and ensured that only true redundancies were targeted in the subsequent deletion stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fb88570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Deleting duplicate image: dataset_working\\10 (nasi goreng)\\images\\292769.jpg\n",
      "   ‚îî‚îÄ deleting label: dataset_working\\10 (nasi goreng)\\labels\\292769.txt\n",
      "‚ùå Deleting duplicate image: dataset_working\\11 (bubur ayam)\\images\\249119.jpg\n",
      "   ‚îî‚îÄ deleting label: dataset_working\\11 (bubur ayam)\\labels\\249119.txt\n",
      "‚ùå Deleting duplicate image: dataset_working\\13 (mie ayam)\\images\\319409.jpg\n",
      "   ‚îî‚îÄ deleting label: dataset_working\\13 (mie ayam)\\labels\\319409.txt\n",
      "‚ùå Deleting duplicate image: dataset_working\\13 (mie ayam)\\images\\319412.jpg\n",
      "   ‚îî‚îÄ deleting label: dataset_working\\13 (mie ayam)\\labels\\319412.txt\n",
      "‚ùå Deleting duplicate image: dataset_working\\13 (mie ayam)\\images\\319639.jpg\n",
      "   ‚îî‚îÄ deleting label: dataset_working\\13 (mie ayam)\\labels\\319639.txt\n",
      "‚ùå Deleting duplicate image: dataset_working\\14 (nasi padang)\\images\\269533.jpg\n",
      "   ‚îî‚îÄ deleting label: dataset_working\\14 (nasi padang)\\labels\\269533.txt\n",
      "‚ùå Deleting duplicate image: dataset_working\\22 (nasi putih)\\images\\9117.jpg\n",
      "   ‚îî‚îÄ deleting label: dataset_working\\22 (nasi putih)\\labels\\9117.txt\n",
      "‚ùå Deleting duplicate image: dataset_working\\22 (nasi putih)\\images\\11161.jpg\n",
      "   ‚îî‚îÄ deleting label: dataset_working\\22 (nasi putih)\\labels\\11161.txt\n",
      "‚ùå Deleting duplicate image: dataset_working\\33 (ikan goreng)\\images\\ikan_train-10-_jpg.rf.8b247a7eae1531418b6d430ca8c72057.jpg\n",
      "   ‚îî‚îÄ deleting label: dataset_working\\33 (ikan goreng)\\labels\\ikan_train-10-_jpg.rf.8b247a7eae1531418b6d430ca8c72057.txt\n",
      "‚ùå Deleting duplicate image: dataset_working\\9 (mie goreng)\\images\\143_271148.jpg\n",
      "   ‚îî‚îÄ deleting label: dataset_working\\9 (mie goreng)\\labels\\143_271148.txt\n",
      "Images deleted : 10\n",
      "Labels deleted : 10\n"
     ]
    }
   ],
   "source": [
    "DATASET_ROOT = Path(\"dataset_working\")\n",
    "LABEL_DIR_NAME = \"labels\"\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "\n",
    "\n",
    "def file_hash(path: Path, chunk_size=8192):\n",
    "    h = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while chunk := f.read(chunk_size):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "deleted_images = 0\n",
    "deleted_labels = 0\n",
    "\n",
    "for cls in sorted(DATASET_ROOT.iterdir()):\n",
    "    if not cls.is_dir():\n",
    "        continue\n",
    "\n",
    "    img_dir = cls / \"images\"\n",
    "    lbl_dir = cls / LABEL_DIR_NAME\n",
    "    if not img_dir.exists() or not lbl_dir.exists():\n",
    "        continue\n",
    "\n",
    "    hash_map = defaultdict(list)\n",
    "\n",
    "    for img in img_dir.iterdir():\n",
    "        if img.suffix.lower() in IMG_EXTS:\n",
    "            h = file_hash(img)\n",
    "            hash_map[h].append(img)\n",
    "\n",
    "    for imgs in hash_map.values():\n",
    "        if len(imgs) > 1:\n",
    "            keep = imgs[0]\n",
    "\n",
    "            for dup_img in imgs[1:]:\n",
    "                lbl_path = lbl_dir / f\"{dup_img.stem}.txt\"\n",
    "\n",
    "                print(f\"‚ùå Deleting duplicate image: {dup_img}\")\n",
    "                dup_img.unlink()\n",
    "                deleted_images += 1\n",
    "\n",
    "                if lbl_path.exists():\n",
    "                    print(f\"   ‚îî‚îÄ deleting label: {lbl_path}\")\n",
    "                    lbl_path.unlink()\n",
    "                    deleted_labels += 1\n",
    "\n",
    "print(f\"Images deleted : {deleted_images}\")\n",
    "print(f\"Labels deleted : {deleted_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3447ce",
   "metadata": {},
   "source": [
    "Following verification, confirmed duplicate images were removed while retaining a single representative copy per duplicate group. This reduced dataset redundancy without altering class semantics or introducing annotation inconsistencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26aadfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images scanned : 6578\n",
      "Total labels scanned : 6578\n",
      "----------------------------------------------------------------------------------------------------\n",
      "‚ùå Missing labels (0):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "‚ùå Missing images (0):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "‚ö† Empty label files (0):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "‚ùå Invalid YOLO entries (0):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "‚ùå Corrupted images (0):\n",
      "----------------------------------------------------------------------------------------------------\n",
      "‚ö† Duplicate image groups (PER CLASS):\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "DATASET_ROOT = Path(r\"dataset_working\")\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "\n",
    "\n",
    "# MD5 hash ‚Üí detects exact binary duplicates (content-based)\n",
    "# ------------------------------\n",
    "def file_hash(path: Path, chunk_size=8192):\n",
    "    h = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while chunk := f.read(chunk_size):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def read_yolo_lines(label_path: Path):\n",
    "    with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [ln.strip() for ln in f if ln.strip()]\n",
    "\n",
    "\n",
    "missing_label = []\n",
    "missing_image = []\n",
    "empty_labels = []\n",
    "invalid_yolo = []\n",
    "corrupted_images = []\n",
    "\n",
    "# per-class image hashes\n",
    "image_hashes = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "total_images = 0\n",
    "total_labels = 0\n",
    "\n",
    "\n",
    "for cls in sorted(\n",
    "    [p for p in DATASET_ROOT.iterdir() if p.is_dir() and not p.name.startswith(\"_\")],\n",
    "    key=lambda p: p.name.lower()\n",
    "):\n",
    "    class_name = cls.name\n",
    "\n",
    "    img_dir = cls / \"images\"\n",
    "    lbl_dir = cls / \"labels\"\n",
    "    if not img_dir.exists() or not lbl_dir.exists():\n",
    "        continue\n",
    "\n",
    "    images = [p for p in img_dir.iterdir() if p.suffix.lower() in IMG_EXTS]\n",
    "    labels = list(lbl_dir.glob(\"*.txt\"))\n",
    "\n",
    "    total_images += len(images)\n",
    "    total_labels += len(labels)\n",
    "\n",
    "    img_map = {p.stem: p for p in images}\n",
    "    lbl_map = {p.stem: p for p in labels}\n",
    "\n",
    "\n",
    "    # Missing pairs\n",
    "    # -------------------------\n",
    "    for stem, img_path in img_map.items():\n",
    "        if stem not in lbl_map:\n",
    "            missing_label.append(str(img_path.resolve()))\n",
    "\n",
    "    for stem, lbl_path in lbl_map.items():\n",
    "        if stem not in img_map:\n",
    "            missing_image.append(str(lbl_path.resolve()))\n",
    "\n",
    "\n",
    "    # Image checks + per-class duplicates\n",
    "    # -------------------------\n",
    "    for img_path in images:\n",
    "        try:\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None or img.size == 0:\n",
    "                corrupted_images.append(str(img_path.resolve()))\n",
    "                continue\n",
    "        except Exception:\n",
    "            corrupted_images.append(str(img_path.resolve()))\n",
    "            continue\n",
    "\n",
    "        h = file_hash(img_path)\n",
    "        image_hashes[class_name][h].append(str(img_path.resolve()))\n",
    "\n",
    "\n",
    "    # Label checks (NO duplicate detection)\n",
    "    # -------------------------\n",
    "    for lbl_path in labels:\n",
    "        lines = read_yolo_lines(lbl_path)\n",
    "\n",
    "        if len(lines) == 0:\n",
    "            empty_labels.append(str(lbl_path.resolve()))\n",
    "            continue\n",
    "\n",
    "        for ln in lines:\n",
    "            parts = ln.split()\n",
    "            if len(parts) != 5:\n",
    "                invalid_yolo.append((str(lbl_path.resolve()), ln))\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                _, x, y, w, h = parts\n",
    "                x, y, w, h = map(float, (x, y, w, h))\n",
    "            except Exception:\n",
    "                invalid_yolo.append((str(lbl_path.resolve()), ln))\n",
    "                continue\n",
    "\n",
    "            if not (0 < x <= 1 and 0 < y <= 1 and 0 < w <= 1 and 0 < h <= 1):\n",
    "                invalid_yolo.append((str(lbl_path.resolve()), ln))\n",
    "\n",
    "\n",
    "# PER-CLASS DUPLICATES\n",
    "# =========================\n",
    "dup_images = {}\n",
    "\n",
    "for cls_name, hashes in image_hashes.items():\n",
    "    for h, files in hashes.items():\n",
    "        if len(files) > 1:\n",
    "            dup_images.setdefault(cls_name, []).append(files)\n",
    "\n",
    "\n",
    "# FULL REPORT\n",
    "# =========================\n",
    "\n",
    "print(f\"Total images scanned : {total_images}\")\n",
    "print(f\"Total labels scanned : {total_labels}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(f\"‚ùå Missing labels ({len(missing_label)}):\")\n",
    "for p in missing_label:\n",
    "    print(p)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(f\"‚ùå Missing images ({len(missing_image)}):\")\n",
    "for p in missing_image:\n",
    "    print(p)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(f\"‚ö† Empty label files ({len(empty_labels)}):\")\n",
    "for p in empty_labels:\n",
    "    print(p)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(f\"‚ùå Invalid YOLO entries ({len(invalid_yolo)}):\")\n",
    "for p, ln in invalid_yolo:\n",
    "    print(f\"{p}  |  {ln}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(f\"‚ùå Corrupted images ({len(corrupted_images)}):\")\n",
    "for p in corrupted_images:\n",
    "    print(p)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(\"‚ö† Duplicate image groups (PER CLASS):\")\n",
    "for cls_name, groups in dup_images.items():\n",
    "    print(f\"\\nClass: {cls_name}\")\n",
    "    for group in groups:\n",
    "        for f in group:\n",
    "            print(\" \", f)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af88167",
   "metadata": {},
   "source": [
    "6. Dataset Integrity Check (Re-EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ce7bc",
   "metadata": {},
   "source": [
    "A second exploratory analysis was conducted after preprocessing to assess the effective dataset distribution. No additional dataset balancing or filtering was applied, as class frequencies, image resolutions, and bounding box scales were within acceptable ranges. Images were retained at their original resolutions to preserve visual detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ac270935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATASET SUMMARY ===\n",
      "Classes       : 40\n",
      "Images        : 6578\n",
      "Bounding boxes: 11100\n"
     ]
    }
   ],
   "source": [
    "total_images = 0\n",
    "total_boxes = 0\n",
    "class_names = []\n",
    "\n",
    "for cls in DATASET_ROOT.iterdir():\n",
    "    if not cls.is_dir():\n",
    "        continue\n",
    "\n",
    "    img_dir = cls / \"images\"\n",
    "    lbl_dir = cls / \"labels\"\n",
    "    if not img_dir.exists() or not lbl_dir.exists():\n",
    "        continue\n",
    "\n",
    "    images = [p for p in img_dir.iterdir() if p.suffix.lower() in IMG_EXTS]\n",
    "    labels = list(lbl_dir.glob(\"*.txt\"))\n",
    "\n",
    "    total_images += len(images)\n",
    "    class_names.append(cls.name)\n",
    "\n",
    "    for lbl in labels:\n",
    "        total_boxes += len(lbl.read_text().strip().splitlines())\n",
    "\n",
    "print(\"=== DATASET SUMMARY ===\")\n",
    "print(\"Classes       :\", len(class_names))\n",
    "print(\"Images        :\", total_images)\n",
    "print(\"Bounding boxes:\", total_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9eb46a",
   "metadata": {},
   "source": [
    "6.1 Image per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e2c50fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_1f423\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_1f423_level0_col0\" class=\"col_heading level0 col0\" >class</th>\n",
       "      <th id=\"T_1f423_level0_col1\" class=\"col_heading level0 col1\" >images</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row0_col0\" class=\"data row0 col0\" >10 (nasi goreng)</td>\n",
       "      <td id=\"T_1f423_row0_col1\" class=\"data row0 col1\" >326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row1_col0\" class=\"data row1 col0\" >4 (sate)</td>\n",
       "      <td id=\"T_1f423_row1_col1\" class=\"data row1 col1\" >321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row2_col0\" class=\"data row2 col0\" >2 (rendang)</td>\n",
       "      <td id=\"T_1f423_row2_col1\" class=\"data row2 col1\" >299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row3_col0\" class=\"data row3 col0\" >31 (pempek)</td>\n",
       "      <td id=\"T_1f423_row3_col1\" class=\"data row3 col1\" >298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row4_col0\" class=\"data row4 col0\" >5 (bakso)</td>\n",
       "      <td id=\"T_1f423_row4_col1\" class=\"data row4 col1\" >293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row5_col0\" class=\"data row5 col0\" >9 (mie goreng)</td>\n",
       "      <td id=\"T_1f423_row5_col1\" class=\"data row5 col1\" >240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row6_col0\" class=\"data row6 col0\" >22 (nasi putih)</td>\n",
       "      <td id=\"T_1f423_row6_col1\" class=\"data row6 col1\" >230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row7_col0\" class=\"data row7 col0\" >34 (telur rebus)</td>\n",
       "      <td id=\"T_1f423_row7_col1\" class=\"data row7 col1\" >200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row8_col0\" class=\"data row8 col0\" >21 (telur ceplok)</td>\n",
       "      <td id=\"T_1f423_row8_col1\" class=\"data row8 col1\" >199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row9_col0\" class=\"data row9 col0\" >24 (putu ayu)</td>\n",
       "      <td id=\"T_1f423_row9_col1\" class=\"data row9 col1\" >190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row10_col0\" class=\"data row10 col0\" >25 (kue cubit)</td>\n",
       "      <td id=\"T_1f423_row10_col1\" class=\"data row10 col1\" >189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row11_col0\" class=\"data row11 col0\" >23 (dadar gulung)</td>\n",
       "      <td id=\"T_1f423_row11_col1\" class=\"data row11 col1\" >189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row12_col0\" class=\"data row12 col0\" >26 (pepes ikan)</td>\n",
       "      <td id=\"T_1f423_row12_col1\" class=\"data row12 col1\" >186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row13_col0\" class=\"data row13 col0\" >19 (telur balado)</td>\n",
       "      <td id=\"T_1f423_row13_col1\" class=\"data row13 col1\" >186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row14_col0\" class=\"data row14 col0\" >6 (ayam bakar)</td>\n",
       "      <td id=\"T_1f423_row14_col1\" class=\"data row14 col1\" >184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row15_col0\" class=\"data row15 col0\" >27 (bika ambon)</td>\n",
       "      <td id=\"T_1f423_row15_col1\" class=\"data row15 col1\" >183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row16_col0\" class=\"data row16 col0\" >37 (tempe bacem)</td>\n",
       "      <td id=\"T_1f423_row16_col1\" class=\"data row16 col1\" >182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row17_col0\" class=\"data row17 col0\" >30 (bihun goreng)</td>\n",
       "      <td id=\"T_1f423_row17_col1\" class=\"data row17 col1\" >149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row18_col0\" class=\"data row18 col0\" >14 (nasi padang)</td>\n",
       "      <td id=\"T_1f423_row18_col1\" class=\"data row18 col1\" >147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row19_col0\" class=\"data row19 col0\" >35 (martabak manis)</td>\n",
       "      <td id=\"T_1f423_row19_col1\" class=\"data row19 col1\" >142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row20_col0\" class=\"data row20 col0\" >7 (ayam goreng)</td>\n",
       "      <td id=\"T_1f423_row20_col1\" class=\"data row20 col1\" >135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row21_col0\" class=\"data row21 col0\" >12 (cakwe)</td>\n",
       "      <td id=\"T_1f423_row21_col1\" class=\"data row21 col1\" >135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row22_col0\" class=\"data row22 col0\" >33 (ikan goreng)</td>\n",
       "      <td id=\"T_1f423_row22_col1\" class=\"data row22 col1\" >124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row23_col0\" class=\"data row23 col0\" >32 (batagor)</td>\n",
       "      <td id=\"T_1f423_row23_col1\" class=\"data row23 col1\" >122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row24_col0\" class=\"data row24 col0\" >3 (kangkung)</td>\n",
       "      <td id=\"T_1f423_row24_col1\" class=\"data row24 col1\" >120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row25_col0\" class=\"data row25 col0\" >17 (nasi babi campur)</td>\n",
       "      <td id=\"T_1f423_row25_col1\" class=\"data row25 col1\" >117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row26_col0\" class=\"data row26 col0\" >38 (terong balado)</td>\n",
       "      <td id=\"T_1f423_row26_col1\" class=\"data row26 col1\" >115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row27_col0\" class=\"data row27 col0\" >20 (telur dadar)</td>\n",
       "      <td id=\"T_1f423_row27_col1\" class=\"data row27 col1\" >114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row28_col0\" class=\"data row28 col0\" >36 (gulai ikan)</td>\n",
       "      <td id=\"T_1f423_row28_col1\" class=\"data row28 col1\" >110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row29_col0\" class=\"data row29 col0\" >29 (lumpia)</td>\n",
       "      <td id=\"T_1f423_row29_col1\" class=\"data row29 col1\" >110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row30_col0\" class=\"data row30 col0\" >28 (soto)</td>\n",
       "      <td id=\"T_1f423_row30_col1\" class=\"data row30 col1\" >110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row31_col0\" class=\"data row31 col0\" >15 (babi guling)</td>\n",
       "      <td id=\"T_1f423_row31_col1\" class=\"data row31 col1\" >108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row32_col0\" class=\"data row32 col0\" >11 (bubur ayam)</td>\n",
       "      <td id=\"T_1f423_row32_col1\" class=\"data row32 col1\" >107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row33_col0\" class=\"data row33 col0\" >1 (tahu goreng)</td>\n",
       "      <td id=\"T_1f423_row33_col1\" class=\"data row33 col1\" >106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row34_col0\" class=\"data row34 col0\" >16 (nasi uduk)</td>\n",
       "      <td id=\"T_1f423_row34_col1\" class=\"data row34 col1\" >106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row35_col0\" class=\"data row35 col0\" >8 (gado gado)</td>\n",
       "      <td id=\"T_1f423_row35_col1\" class=\"data row35 col1\" >105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row36_col0\" class=\"data row36 col0\" >39 (bakwan)</td>\n",
       "      <td id=\"T_1f423_row36_col1\" class=\"data row36 col1\" >104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row37_col0\" class=\"data row37 col0\" >13 (mie ayam)</td>\n",
       "      <td id=\"T_1f423_row37_col1\" class=\"data row37 col1\" >104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row38_col0\" class=\"data row38 col0\" >18 (ayam pop)</td>\n",
       "      <td id=\"T_1f423_row38_col1\" class=\"data row38 col1\" >103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_1f423_row39_col0\" class=\"data row39 col0\" >0 (tempe goreng)</td>\n",
       "      <td id=\"T_1f423_row39_col1\" class=\"data row39 col1\" >90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2f4e1925090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_image_counts = []\n",
    "\n",
    "for cls in DATASET_ROOT.iterdir():\n",
    "    if not cls.is_dir():\n",
    "        continue\n",
    "\n",
    "    img_dir = cls / \"images\"\n",
    "    if not img_dir.exists():\n",
    "        continue\n",
    "\n",
    "    count = sum(1 for p in img_dir.iterdir() if p.suffix.lower() in IMG_EXTS)\n",
    "    class_image_counts.append({\n",
    "        \"class\": cls.name,\n",
    "        \"images\": count\n",
    "    })\n",
    "\n",
    "df_images_per_class = (\n",
    "    pd.DataFrame(class_image_counts)\n",
    "    .sort_values(\"images\", ascending=False)\n",
    ")\n",
    "\n",
    "display(df_images_per_class.style.hide(axis=\"index\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca19a245",
   "metadata": {},
   "source": [
    "6.2 Bounding Box per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bff117bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_715ad\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_715ad_level0_col0\" class=\"col_heading level0 col0\" >class</th>\n",
       "      <th id=\"T_715ad_level0_col1\" class=\"col_heading level0 col1\" >boxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row0_col0\" class=\"data row0 col0\" >25 (kue cubit)</td>\n",
       "      <td id=\"T_715ad_row0_col1\" class=\"data row0 col1\" >1193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row1_col0\" class=\"data row1 col0\" >24 (putu ayu)</td>\n",
       "      <td id=\"T_715ad_row1_col1\" class=\"data row1 col1\" >1169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row2_col0\" class=\"data row2 col0\" >31 (pempek)</td>\n",
       "      <td id=\"T_715ad_row2_col1\" class=\"data row2 col1\" >641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row3_col0\" class=\"data row3 col0\" >37 (tempe bacem)</td>\n",
       "      <td id=\"T_715ad_row3_col1\" class=\"data row3 col1\" >568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row4_col0\" class=\"data row4 col0\" >1 (tahu goreng)</td>\n",
       "      <td id=\"T_715ad_row4_col1\" class=\"data row4 col1\" >426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row5_col0\" class=\"data row5 col0\" >0 (tempe goreng)</td>\n",
       "      <td id=\"T_715ad_row5_col1\" class=\"data row5 col1\" >422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row6_col0\" class=\"data row6 col0\" >34 (telur rebus)</td>\n",
       "      <td id=\"T_715ad_row6_col1\" class=\"data row6 col1\" >422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row7_col0\" class=\"data row7 col0\" >5 (bakso)</td>\n",
       "      <td id=\"T_715ad_row7_col1\" class=\"data row7 col1\" >363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row8_col0\" class=\"data row8 col0\" >4 (sate)</td>\n",
       "      <td id=\"T_715ad_row8_col1\" class=\"data row8 col1\" >344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row9_col0\" class=\"data row9 col0\" >23 (dadar gulung)</td>\n",
       "      <td id=\"T_715ad_row9_col1\" class=\"data row9 col1\" >341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row10_col0\" class=\"data row10 col0\" >10 (nasi goreng)</td>\n",
       "      <td id=\"T_715ad_row10_col1\" class=\"data row10 col1\" >333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row11_col0\" class=\"data row11 col0\" >2 (rendang)</td>\n",
       "      <td id=\"T_715ad_row11_col1\" class=\"data row11 col1\" >310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row12_col0\" class=\"data row12 col0\" >27 (bika ambon)</td>\n",
       "      <td id=\"T_715ad_row12_col1\" class=\"data row12 col1\" >298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row13_col0\" class=\"data row13 col0\" >9 (mie goreng)</td>\n",
       "      <td id=\"T_715ad_row13_col1\" class=\"data row13 col1\" >240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row14_col0\" class=\"data row14 col0\" >22 (nasi putih)</td>\n",
       "      <td id=\"T_715ad_row14_col1\" class=\"data row14 col1\" >231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row15_col0\" class=\"data row15 col0\" >26 (pepes ikan)</td>\n",
       "      <td id=\"T_715ad_row15_col1\" class=\"data row15 col1\" >218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row16_col0\" class=\"data row16 col0\" >21 (telur ceplok)</td>\n",
       "      <td id=\"T_715ad_row16_col1\" class=\"data row16 col1\" >214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row17_col0\" class=\"data row17 col0\" >18 (ayam pop)</td>\n",
       "      <td id=\"T_715ad_row17_col1\" class=\"data row17 col1\" >197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row18_col0\" class=\"data row18 col0\" >6 (ayam bakar)</td>\n",
       "      <td id=\"T_715ad_row18_col1\" class=\"data row18 col1\" >194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row19_col0\" class=\"data row19 col0\" >19 (telur balado)</td>\n",
       "      <td id=\"T_715ad_row19_col1\" class=\"data row19 col1\" >187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row20_col0\" class=\"data row20 col0\" >33 (ikan goreng)</td>\n",
       "      <td id=\"T_715ad_row20_col1\" class=\"data row20 col1\" >180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row21_col0\" class=\"data row21 col0\" >35 (martabak manis)</td>\n",
       "      <td id=\"T_715ad_row21_col1\" class=\"data row21 col1\" >179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row22_col0\" class=\"data row22 col0\" >7 (ayam goreng)</td>\n",
       "      <td id=\"T_715ad_row22_col1\" class=\"data row22 col1\" >172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row23_col0\" class=\"data row23 col0\" >14 (nasi padang)</td>\n",
       "      <td id=\"T_715ad_row23_col1\" class=\"data row23 col1\" >172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row24_col0\" class=\"data row24 col0\" >12 (cakwe)</td>\n",
       "      <td id=\"T_715ad_row24_col1\" class=\"data row24 col1\" >170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row25_col0\" class=\"data row25 col0\" >28 (soto)</td>\n",
       "      <td id=\"T_715ad_row25_col1\" class=\"data row25 col1\" >162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row26_col0\" class=\"data row26 col0\" >39 (bakwan)</td>\n",
       "      <td id=\"T_715ad_row26_col1\" class=\"data row26 col1\" >159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row27_col0\" class=\"data row27 col0\" >20 (telur dadar)</td>\n",
       "      <td id=\"T_715ad_row27_col1\" class=\"data row27 col1\" >149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row28_col0\" class=\"data row28 col0\" >30 (bihun goreng)</td>\n",
       "      <td id=\"T_715ad_row28_col1\" class=\"data row28 col1\" >149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row29_col0\" class=\"data row29 col0\" >36 (gulai ikan)</td>\n",
       "      <td id=\"T_715ad_row29_col1\" class=\"data row29 col1\" >147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row30_col0\" class=\"data row30 col0\" >38 (terong balado)</td>\n",
       "      <td id=\"T_715ad_row30_col1\" class=\"data row30 col1\" >142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row31_col0\" class=\"data row31 col0\" >32 (batagor)</td>\n",
       "      <td id=\"T_715ad_row31_col1\" class=\"data row31 col1\" >125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row32_col0\" class=\"data row32 col0\" >3 (kangkung)</td>\n",
       "      <td id=\"T_715ad_row32_col1\" class=\"data row32 col1\" >122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row33_col0\" class=\"data row33 col0\" >17 (nasi babi campur)</td>\n",
       "      <td id=\"T_715ad_row33_col1\" class=\"data row33 col1\" >117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row34_col0\" class=\"data row34 col0\" >29 (lumpia)</td>\n",
       "      <td id=\"T_715ad_row34_col1\" class=\"data row34 col1\" >110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row35_col0\" class=\"data row35 col0\" >8 (gado gado)</td>\n",
       "      <td id=\"T_715ad_row35_col1\" class=\"data row35 col1\" >109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row36_col0\" class=\"data row36 col0\" >15 (babi guling)</td>\n",
       "      <td id=\"T_715ad_row36_col1\" class=\"data row36 col1\" >108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row37_col0\" class=\"data row37 col0\" >11 (bubur ayam)</td>\n",
       "      <td id=\"T_715ad_row37_col1\" class=\"data row37 col1\" >107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row38_col0\" class=\"data row38 col0\" >16 (nasi uduk)</td>\n",
       "      <td id=\"T_715ad_row38_col1\" class=\"data row38 col1\" >106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_715ad_row39_col0\" class=\"data row39 col0\" >13 (mie ayam)</td>\n",
       "      <td id=\"T_715ad_row39_col1\" class=\"data row39 col1\" >104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2f4e19273d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_box_counts = []\n",
    "\n",
    "for cls in DATASET_ROOT.iterdir():\n",
    "    if not cls.is_dir():\n",
    "        continue\n",
    "\n",
    "    lbl_dir = cls / \"labels\"\n",
    "    if not lbl_dir.exists():\n",
    "        continue\n",
    "\n",
    "    box_count = 0\n",
    "    for lbl in lbl_dir.glob(\"*.txt\"):\n",
    "        box_count += len(lbl.read_text().strip().splitlines())\n",
    "\n",
    "    class_box_counts.append({\n",
    "        \"class\": cls.name,\n",
    "        \"boxes\": box_count\n",
    "    })\n",
    "\n",
    "df_boxes_per_class = (\n",
    "    pd.DataFrame(class_box_counts)\n",
    "    .sort_values(\"boxes\", ascending=False)\n",
    ")\n",
    "\n",
    "display(df_boxes_per_class.style.hide(axis=\"index\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35627ecb",
   "metadata": {},
   "source": [
    "6.3 Image Resolution Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4db72a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_9748e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_9748e_level0_col0\" class=\"col_heading level0 col0\" >width</th>\n",
       "      <th id=\"T_9748e_level0_col1\" class=\"col_heading level0 col1\" >height</th>\n",
       "      <th id=\"T_9748e_level0_col2\" class=\"col_heading level0 col2\" >count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row0_col0\" class=\"data row0 col0\" >500</td>\n",
       "      <td id=\"T_9748e_row0_col1\" class=\"data row0 col1\" >500</td>\n",
       "      <td id=\"T_9748e_row0_col2\" class=\"data row0 col2\" >485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row1_col0\" class=\"data row1 col0\" >640</td>\n",
       "      <td id=\"T_9748e_row1_col1\" class=\"data row1 col1\" >640</td>\n",
       "      <td id=\"T_9748e_row1_col2\" class=\"data row1 col2\" >424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row2_col0\" class=\"data row2 col0\" >577</td>\n",
       "      <td id=\"T_9748e_row2_col1\" class=\"data row2 col1\" >433</td>\n",
       "      <td id=\"T_9748e_row2_col2\" class=\"data row2 col2\" >301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row3_col0\" class=\"data row3 col0\" >751</td>\n",
       "      <td id=\"T_9748e_row3_col1\" class=\"data row3 col1\" >532</td>\n",
       "      <td id=\"T_9748e_row3_col2\" class=\"data row3 col2\" >253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row4_col0\" class=\"data row4 col0\" >800</td>\n",
       "      <td id=\"T_9748e_row4_col1\" class=\"data row4 col1\" >600</td>\n",
       "      <td id=\"T_9748e_row4_col2\" class=\"data row4 col2\" >198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row5_col0\" class=\"data row5 col0\" >512</td>\n",
       "      <td id=\"T_9748e_row5_col1\" class=\"data row5 col1\" >512</td>\n",
       "      <td id=\"T_9748e_row5_col2\" class=\"data row5 col2\" >159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row6_col0\" class=\"data row6 col0\" >500</td>\n",
       "      <td id=\"T_9748e_row6_col1\" class=\"data row6 col1\" >375</td>\n",
       "      <td id=\"T_9748e_row6_col2\" class=\"data row6 col2\" >156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row7_col0\" class=\"data row7 col0\" >680</td>\n",
       "      <td id=\"T_9748e_row7_col1\" class=\"data row7 col1\" >482</td>\n",
       "      <td id=\"T_9748e_row7_col2\" class=\"data row7 col2\" >114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row8_col0\" class=\"data row8 col0\" >1280</td>\n",
       "      <td id=\"T_9748e_row8_col1\" class=\"data row8 col1\" >720</td>\n",
       "      <td id=\"T_9748e_row8_col2\" class=\"data row8 col2\" >108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row9_col0\" class=\"data row9 col0\" >1200</td>\n",
       "      <td id=\"T_9748e_row9_col1\" class=\"data row9 col1\" >630</td>\n",
       "      <td id=\"T_9748e_row9_col2\" class=\"data row9 col2\" >99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row10_col0\" class=\"data row10 col0\" >267</td>\n",
       "      <td id=\"T_9748e_row10_col1\" class=\"data row10 col1\" >189</td>\n",
       "      <td id=\"T_9748e_row10_col2\" class=\"data row10 col2\" >81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row11_col0\" class=\"data row11 col0\" >750</td>\n",
       "      <td id=\"T_9748e_row11_col1\" class=\"data row11 col1\" >500</td>\n",
       "      <td id=\"T_9748e_row11_col2\" class=\"data row11 col2\" >80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row12_col0\" class=\"data row12 col0\" >700</td>\n",
       "      <td id=\"T_9748e_row12_col1\" class=\"data row12 col1\" >393</td>\n",
       "      <td id=\"T_9748e_row12_col2\" class=\"data row12 col2\" >71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row13_col0\" class=\"data row13 col0\" >225</td>\n",
       "      <td id=\"T_9748e_row13_col1\" class=\"data row13 col1\" >225</td>\n",
       "      <td id=\"T_9748e_row13_col2\" class=\"data row13 col2\" >71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row14_col0\" class=\"data row14 col0\" >480</td>\n",
       "      <td id=\"T_9748e_row14_col1\" class=\"data row14 col1\" >360</td>\n",
       "      <td id=\"T_9748e_row14_col2\" class=\"data row14 col2\" >68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row15_col0\" class=\"data row15 col0\" >416</td>\n",
       "      <td id=\"T_9748e_row15_col1\" class=\"data row15 col1\" >416</td>\n",
       "      <td id=\"T_9748e_row15_col2\" class=\"data row15 col2\" >67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row16_col0\" class=\"data row16 col0\" >612</td>\n",
       "      <td id=\"T_9748e_row16_col1\" class=\"data row16 col1\" >408</td>\n",
       "      <td id=\"T_9748e_row16_col2\" class=\"data row16 col2\" >65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row17_col0\" class=\"data row17 col0\" >640</td>\n",
       "      <td id=\"T_9748e_row17_col1\" class=\"data row17 col1\" >480</td>\n",
       "      <td id=\"T_9748e_row17_col2\" class=\"data row17 col2\" >62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row18_col0\" class=\"data row18 col0\" >860</td>\n",
       "      <td id=\"T_9748e_row18_col1\" class=\"data row18 col1\" >610</td>\n",
       "      <td id=\"T_9748e_row18_col2\" class=\"data row18 col2\" >61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_9748e_row19_col0\" class=\"data row19 col0\" >433</td>\n",
       "      <td id=\"T_9748e_row19_col1\" class=\"data row19 col1\" >577</td>\n",
       "      <td id=\"T_9748e_row19_col2\" class=\"data row19 col2\" >51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2f4e1925840>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resolution_counter = Counter()\n",
    "\n",
    "for cls in DATASET_ROOT.iterdir():\n",
    "    if not cls.is_dir():\n",
    "        continue\n",
    "\n",
    "    img_dir = cls / \"images\"\n",
    "    if not img_dir.exists():\n",
    "        continue\n",
    "\n",
    "    for img_path in img_dir.iterdir():\n",
    "        if img_path.suffix.lower() not in IMG_EXTS:\n",
    "            continue\n",
    "\n",
    "        img = cv2.imread(str(img_path))\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        h, w = img.shape[:2]\n",
    "        resolution_counter[(w, h)] += 1\n",
    "\n",
    "df_resolutions = (\n",
    "    pd.DataFrame(\n",
    "        [{\"width\": w, \"height\": h, \"count\": c}\n",
    "         for (w, h), c in resolution_counter.items()]\n",
    "    )\n",
    "    .sort_values(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "display(df_resolutions.head(20).style.hide(axis=\"index\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a51f06",
   "metadata": {},
   "source": [
    "6.4 Box Size Distribution (relative area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "448a8962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       relative_area\n",
      "count   11100.000000\n",
      "mean        0.356062\n",
      "std         0.304503\n",
      "min         0.000006\n",
      "25%         0.089999\n",
      "50%         0.228731\n",
      "75%         0.636353\n",
      "max         1.000000\n"
     ]
    }
   ],
   "source": [
    "box_areas = []\n",
    "\n",
    "for cls in DATASET_ROOT.iterdir():\n",
    "    if not cls.is_dir():\n",
    "        continue\n",
    "\n",
    "    lbl_dir = cls / \"labels\"\n",
    "    if not lbl_dir.exists():\n",
    "        continue\n",
    "\n",
    "    for lbl in lbl_dir.glob(\"*.txt\"):\n",
    "        for ln in lbl.read_text().splitlines():\n",
    "            parts = ln.split()\n",
    "            if len(parts) != 5:\n",
    "                continue\n",
    "            _, _, _, w, h = map(float, parts)\n",
    "            box_areas.append(w * h)\n",
    "\n",
    "df_box_area = pd.DataFrame({\"relative_area\": box_areas})\n",
    "\n",
    "print(df_box_area.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407e896",
   "metadata": {},
   "source": [
    "Following preprocessing and label normalization, exploratory analysis was repeated on the cleaned dataset to verify class distributions, annotation consistency, and overall data integrity. The re-analysis confirmed that all labels conform to the YOLO format with zero-based class indexing and that no structural or annotation anomalies remain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd85070",
   "metadata": {},
   "source": [
    "7. Splitting Train/Val/Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783156f5",
   "metadata": {},
   "source": [
    "The dataset was split into training, validation, and test sets using a 70/15/15 ratio. Stratified sampling was applied at the class level to ensure that each food category was represented across all subsets. A fixed random seed (42) was used to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "04dd0852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è DATASET SPLIT ‚Äî YOLO FORMAT (70/15/15, Stratified per class)\n",
      "\n",
      "Class 0 (tempe goreng): 90 ‚Üí train=62, val=13, test=15\n",
      "Class 1 (tahu goreng): 106 ‚Üí train=74, val=15, test=17\n",
      "Class 10 (nasi goreng): 326 ‚Üí train=228, val=48, test=50\n",
      "Class 11 (bubur ayam): 107 ‚Üí train=74, val=16, test=17\n",
      "Class 12 (cakwe): 135 ‚Üí train=94, val=20, test=21\n",
      "Class 13 (mie ayam): 104 ‚Üí train=72, val=15, test=17\n",
      "Class 14 (nasi padang): 147 ‚Üí train=102, val=22, test=23\n",
      "Class 15 (babi guling): 108 ‚Üí train=75, val=16, test=17\n",
      "Class 16 (nasi uduk): 106 ‚Üí train=74, val=15, test=17\n",
      "Class 17 (nasi babi campur): 117 ‚Üí train=81, val=17, test=19\n",
      "Class 18 (ayam pop): 103 ‚Üí train=72, val=15, test=16\n",
      "Class 19 (telur balado): 186 ‚Üí train=130, val=27, test=29\n",
      "Class 2 (rendang): 299 ‚Üí train=209, val=44, test=46\n",
      "Class 20 (telur dadar): 114 ‚Üí train=79, val=17, test=18\n",
      "Class 21 (telur ceplok): 199 ‚Üí train=139, val=29, test=31\n",
      "Class 22 (nasi putih): 230 ‚Üí train=161, val=34, test=35\n",
      "Class 23 (dadar gulung): 189 ‚Üí train=132, val=28, test=29\n",
      "Class 24 (putu ayu): 190 ‚Üí train=133, val=28, test=29\n",
      "Class 25 (kue cubit): 189 ‚Üí train=132, val=28, test=29\n",
      "Class 26 (pepes ikan): 186 ‚Üí train=130, val=27, test=29\n",
      "Class 27 (bika ambon): 183 ‚Üí train=128, val=27, test=28\n",
      "Class 28 (soto): 110 ‚Üí train=77, val=16, test=17\n",
      "Class 29 (lumpia): 110 ‚Üí train=77, val=16, test=17\n",
      "Class 3 (kangkung): 120 ‚Üí train=84, val=18, test=18\n",
      "Class 30 (bihun goreng): 149 ‚Üí train=104, val=22, test=23\n",
      "Class 31 (pempek): 298 ‚Üí train=208, val=44, test=46\n",
      "Class 32 (batagor): 122 ‚Üí train=85, val=18, test=19\n",
      "Class 33 (ikan goreng): 124 ‚Üí train=86, val=18, test=20\n",
      "Class 34 (telur rebus): 200 ‚Üí train=140, val=30, test=30\n",
      "Class 35 (martabak manis): 142 ‚Üí train=99, val=21, test=22\n",
      "Class 36 (gulai ikan): 110 ‚Üí train=77, val=16, test=17\n",
      "Class 37 (tempe bacem): 182 ‚Üí train=127, val=27, test=28\n",
      "Class 38 (terong balado): 115 ‚Üí train=80, val=17, test=18\n",
      "Class 39 (bakwan): 104 ‚Üí train=72, val=15, test=17\n",
      "Class 4 (sate): 321 ‚Üí train=224, val=48, test=49\n",
      "Class 5 (bakso): 293 ‚Üí train=205, val=43, test=45\n",
      "Class 6 (ayam bakar): 184 ‚Üí train=128, val=27, test=29\n",
      "Class 7 (ayam goreng): 135 ‚Üí train=94, val=20, test=21\n",
      "Class 8 (gado gado): 105 ‚Üí train=73, val=15, test=17\n",
      "Class 9 (mie goreng): 240 ‚Üí train=168, val=36, test=36\n",
      "\n",
      "‚úÖ SPLIT COMPLETE (YOLO READY)\n",
      "Train images : 4589\n",
      "Val images   : 968\n",
      "Test images  : 1021\n",
      "Random seed  : 42\n"
     ]
    }
   ],
   "source": [
    "SRC_ROOT = Path(\"dataset_working\")\n",
    "DST_ROOT = Path(\"dataset_final\")\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\"}\n",
    "SEED = 42\n",
    "\n",
    "TRAIN_RATIO = 0.70\n",
    "VAL_RATIO   = 0.15\n",
    "TEST_RATIO  = 0.15\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "# CREATE YOLO DIR STRUCTURE\n",
    "# =========================\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    (DST_ROOT / split / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "    (DST_ROOT / split / \"labels\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÇÔ∏è DATASET SPLIT ‚Äî YOLO FORMAT (70/15/15, Stratified per class)\\n\")\n",
    "\n",
    "total_counts = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "\n",
    "# SPLIT PER CLASS (STRATIFIED)\n",
    "# =========================\n",
    "for cls in sorted(SRC_ROOT.iterdir()):\n",
    "    if not cls.is_dir():\n",
    "        continue\n",
    "\n",
    "    img_dir = cls / \"images\"\n",
    "    lbl_dir = cls / \"labels\"\n",
    "\n",
    "    if not img_dir.exists() or not lbl_dir.exists():\n",
    "        continue\n",
    "\n",
    "    # Collect valid image‚Äìlabel pairs\n",
    "    pairs = []\n",
    "    for img_path in img_dir.iterdir():\n",
    "        if img_path.suffix.lower() not in IMG_EXTS:\n",
    "            continue\n",
    "\n",
    "        lbl_path = lbl_dir / f\"{img_path.stem}.txt\"\n",
    "        if lbl_path.exists():\n",
    "            pairs.append((img_path, lbl_path))\n",
    "\n",
    "    if not pairs:\n",
    "        continue\n",
    "\n",
    "    random.shuffle(pairs)\n",
    "\n",
    "    n = len(pairs)\n",
    "    n_train = int(n * TRAIN_RATIO)\n",
    "    n_val   = int(n * VAL_RATIO)\n",
    "\n",
    "    splits = {\n",
    "        \"train\": pairs[:n_train],\n",
    "        \"val\":   pairs[n_train:n_train + n_val],\n",
    "        \"test\":  pairs[n_train + n_val:]\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"Class {cls.name}: {n} ‚Üí \"\n",
    "        f\"train={len(splits['train'])}, \"\n",
    "        f\"val={len(splits['val'])}, \"\n",
    "        f\"test={len(splits['test'])}\"\n",
    "    )\n",
    "\n",
    "    for split_name, items in splits.items():\n",
    "        for img_path, lbl_path in items:\n",
    "            shutil.copy2(\n",
    "                img_path,\n",
    "                DST_ROOT / split_name / \"images\" / img_path.name\n",
    "            )\n",
    "            shutil.copy2(\n",
    "                lbl_path,\n",
    "                DST_ROOT / split_name / \"labels\" / lbl_path.name\n",
    "            )\n",
    "            total_counts[split_name] += 1\n",
    "\n",
    "\n",
    "print(\"\\n‚úÖ SPLIT COMPLETE (YOLO READY)\")\n",
    "print(f\"Train images : {total_counts['train']}\")\n",
    "print(f\"Val images   : {total_counts['val']}\")\n",
    "print(f\"Test images  : {total_counts['test']}\")\n",
    "print(f\"Random seed  : {SEED}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compvis (3.10.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
